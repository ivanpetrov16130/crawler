# crawler

Just primitive web-crawler, which helps you to get all the "usable" urls from some pages.

## Installation

WTF is installation? I'm noob, download it yourself and compile!

## Usage

Lol, run it in lein and open your found urls recorded in file("outurls.txt").

    $ cd MyWayToClojureProjects/crawler
    $ cd lein run -YourLevelOfPenetration

## Options

--YourLevelOfPenetration
How many times crawler should process bunches of urls? That's it!

## Examples

I'm so tired of this, try your own.

### Bugs

¯\_(ツ)_/¯

### That You Think

Law doesn't work, Authorities are illegal, Society is rotting.

## License

Copyright © 2015 FIXME

Distributed under the Eclipse Public License either version 1.0 or (at
your option) any later version.
